# Optimal-Transport-papers
## Application
+ [IJCAI-20] ***Metric Learning in Optimal Transport for Domain Adaptation***  
Domain Adaptation aims at benefiting from a labeled dataset drawn from a source distribution to learn a model from examples generated according to a different but related target distribution. In this paper, we propose to use *Optimal Transport*(OT) and its associated Wasserstein Distance to perform this alignment.
+ [IEEE-20] ***Optimal Transport in Reproducing Kernel Hilbert Spaces: Theory and Applications***  
In this paper, we present a mathematical and computational framework for comparing and matching distributions in reproducing kernel Hilbert Spaces(RKHS). This framework, called *Optimal Transport* in RKHS, is a generalization of the optimal transport problem in input spaces to (potentially) infinite-dimensional feature spaces.
+ [Journal of Machine Learning Research-20] ***Learning to Match via Inverse Optimal Transport***  
We propose a unified data-driven framework based on inverse *Optimal Transport* that can learn adaptive, nonlinear interaction cost function from noisy and incomplete empirical matching matrix and predict new matching in various matching contexts.
+ [AAAI-19] ***Guiding the One-to-One Mapping in CycleGAN via Optimal Transport***  
CycleGAN is capable of learning a one-to-one mapping between two data distributions without paired examples, achieving the task of unsupervised data translation. In this paper, we experimentally find that, under some circumstances, the one-to-one mapping learned by CycleGAN is just a random one within the large feasible solution space. We propose to solve an *Optimal Transport* mapping restrained by a task-specific cost function that reflects the desired properties, and use the barycenters of optimal transport mapping to serve as references for CycleGAN.
+ [IJCAI-19] ***Differentially Private Optimal Transport: Application to Domain Adaptation***  
In this paper, we address the challenging task of privacy preserving domain adaptation by *Optimal Transport*. Using the Johnson-Lindenstrauss transform together with some noise, we present the first differentially private optimal transport model and show how it can be directly applied on both unsupervised and semi-supervised domain adaptation scenarios.
+ [IJCAI-19] ***Improving Cross-lingual Entity Alignment via Optimal Transport***  
Cross-lingual entity alignment identifies entity pairs that share the same meanings but locate in different language knowledge graphs(KGs). We propose a novel entity alignment framework (OTEA), which dually optimizes the entity-level loss and group-level loss via *Optimal Transport* theory. We also impose a regularizer on the dual translation matrices to mitigate the effect of noise during transformation.
+ [ICLR-19] ***Improving Sequence-to-Sequence Learning via Optimal Transport***  
Sequence-to-Sequence models are commonly trained via maximum likelihood estimation(MLE). However, standard MLE training considers a word-level objective, predicting the next word given the previous ground-truth partial sentence. Our approach imposes global sequence-level guidance via new supervision based on *Optimal Transport*, enabling the overall characterization and preservation of semantic features.
+ [AAAI-18] ***Label Distribution Learning by Optimal Transport***  
Label distribution learning(LDL) is a novel learning paradigm to deal with some real-world applications, especially when we care about the relative importance of different labels in description of an instance. In this paper, we proposed an approach to learn the label distribution and exploit label correlations simultaneously based on the *Optimal Transport(OT)* theory. Besides, we provide perhaps the first data-dependence risk bound analysis for label distribution learning by Sinkhorn distance, a commonly-used relaxation for OT distance.
+ [IJCAI-18] ***Semi-Supervised Optimal Transport for Heterogeneous Domain Adaptation***  
Heterogeneous domain adaptation(HDA) aims to exploit knowledge from a heterogeneous source domain to improve the learning performance in a target domain. In this paper, we propose a novel semi-supervised algorithm for HDA by exploiting the theory of *Optimal Transport*(OT), a powerful tool originally designed for aligning two different distributions.
+ [ICLR-18] ***Improving GANs Using Optimal Transport***  
We present Optimal Transport GAN(OT-GAN), a variant of generative adversarial nets minimizing a new metric measuring the distance between the generator distribution and the data distribution. Experimentally we show OT-GAN to be highly stable when trained with large mini-batches, and we present state-of-the-art results on several popular benchmark problems for image generation.

## Algorithm
+ [ICLR-19] ***Scalable Unbalanced Optimal Transport Using Generative Adversarial Networks***  
In this paper, we present a scalable method for unbalanced optimal transport(OT) based on the generative-adversarial framework. We formulate unbalanced OT as a problem of simultaneously learning a transport map and a scaling factor that push a source measure to a target measure in a cost-optimal manner.
+ [NeurlPS-19] ***Discriminator Optimal Transport***
Within a broad class of generative adversarial networks, we show that discriminator optimization process increases a lower bound of the dual cost function for the Wasserstein distance between the target distribution $p$ and the generator distribution $p_{G}$. Based on some experiments and a bit of *OT* theory, we propose discriminator optimal transport(DOT) scheme to improve generated images.
+ [ICML-18] ***Computational Optimal Transport: Complexity by Accelerated Gradient Descent Is Better Than by Sinkhorn's Algorithm***  
We analyze two algorithms for approximating the general *Optimal Transport*(OT) distance between two discrete distributions of size $n$, up to accuracy $\varepsilon$. For the fist algorithm, which is based on the celebrated Sinkhorn's algorithm. For the second one, which is based on our novel Adaptive Primal-Dual Accelerated Gradient Descent(APDAGD) algorithm. Our second algorithm not only has better dependence on $\varepsilon$ in the complexity bound, but also is not specific to entropic regularization and can solve the OT problem with different regularizers.
+ [ICLR-18] ***Large-Scale Optimal Transport And Mapping Estimation***  
This paper presents a novel two-step approach for the fundamental problem of learning an optimal map from one distribution to another. First, we learn an *Optimal Transport*(OT) plan, which can be thought as a one-to-many map between the two distributions. Second, we estimate a Monge map as a deep neural network learned by approximating the barycentric projection of the previously-obtained OT plan. We showcase our proposed approach on two applications: domain adaptation and generative modeling.

## Theory
+ [AISTATS-18] ***Smooth and Sparse Optimal Transport***  
In this paper, we explore regularizing the primal and dual OT formulations with a strongly convex term, which corresponds to relaxing the dual and primal constraints with smooth approximations. We show how to incorporate squared 2-norm and group lasso regularizations within that framework, leading to sparse and group-sparse transportation plans.
+ [NeurlPS-18] ***On the Convergence and Robustness of Training GANs with Regularized Optimal Transport***  
In this work, we show that obtaining gradient information of the smoothed Wasserstein GAN formulation, which is based on regularized *Optimal Transport*(OT), is computationally effortless and hence one can apply first order optimization methods to minimize this objective. Consequently, we establish theoretical convergence guarantee to stationarity for a proposed class of GAN optimization algorithms.
+ [NeurlPS-18] ***Interpolating between Optimal Transport and MMD using Sinkhorn Divergences***  
This paper studies the Sinkhorn divergences, a family of geometric divergences that interpolates between MMD and OT. Relying on a new notion of geometric entropy, we provide theoretical guarantees for these divergences: positivity, convexity and metrization of the convergence in law.

